# Universal Sentence Encoder

### What contribution？

Main contribution for this paper is proposed two ways under **multi-task learning** setting to get better sentence representation.

1. First model is a Transformer model, which need more resources;
2. About second model, instead of Transformer they use a deep average network (DAN) model to train sentence embedding.

### Why it worth to explore this question？

Better sentence representation is essential for high performance transfer learning.

### What challenge for this question

I think is how to train the model under many tasks.