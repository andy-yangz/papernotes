# Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures

TLDR

### What contribution？

Mainly contributions:

1. Refute CNN or Transformer can learn better long dependency by doing experiments on subject-verb task.
2. Show Transformer can learn better semantic feature through experiments on word sense disambiguation task.
3. Number of attention heads influence the ability to capture long dependency of Transformer.

### Why it worth to explore this question？

Because although there are several works show the better performance of Transformer, and declare that it can learn better long dependency than RNN, there is no actually evidence show this.

### What challenge for this question

Choose exact tasks to test long dependency capture ability.



